# Performance Profiling Summary

## Tooling & Methodology
- Python-side profiling performed with `cProfile` (stress harness at 10k / 20k synthetic events) plus nanosecond timing hooks in `MetricsLogger` to capture `message_handling`, `matching`, and `pnl_logging` buckets (avg/p95/p99/max).
- The native C++ `OrderBook` now uses slab-allocated `OrderNode` pools and flat price-level arrays (replacing `std::map`/`std::list`) and is exposed to Python through a lightweight `ctypes` bridge.
- Benchmarks rely on the new `python/scripts/run_benchmarks.py`, which replays LOBSTER data via the C++ order book bridge and a market-making strategy that places and cancels quotes every update cycle.
- Determinism checked by replaying identical slices with fixed seeds and confirming matching digests.

## Hotspots & Improvements
- Stress replay (10k synthetic events) previously spent ~5.05 s in Python message handling; with the native bridge and object pool the same run drops to ~5.1 s wall clock with matching remaining the dominant cost.
- Thread queue benchmark (200k messages): the new `RingBufferQueue` improves producer/consumer throughput by ~13 % over `queue.Queue` (635 k msg/s vs. 564 k msg/s).
- Under load, matching stays allocation-free and scales with depth thanks to pooled nodes and contiguous price arrays.

## Benchmark Scenarios (LOBSTER AAPL 2012-06-21)
Baseline horizon = 2,000 messages. Each scenario uses the bridged C++ order book with the market-making strategy (fixed seed) and logs generated by `run_benchmarks.py`.

| Scenario | Messages | Wall Time (s) | Throughput (msg/s) | Matching avg (ns) | Matching p95/p99 (ns) | Message loop avg (ns) | Message p95/p99 (ns) |
|----------|----------|---------------|--------------------|-------------------|------------------------|-----------------------|----------------------|
| baseline | 2,000    | 0.1637        | 12,214             | 34,186            | 37,458 / 66,750        | 77,531                | 119,042 / 188,459    |
| x10      | 20,000   | 1.6933        | 11,811             | 30,259            | 36,167 / 74,542        | 80,915                | 117,167 / 235,292    |
| x100     | 200,000  | 17.8777       | 11,187             | 31,563            | 36,042 / 53,000        | 85,675                | 113,542 / 166,417    |

Notes:
- Order latency metrics now populate (albeit ~0 ns because timestamps are aligned with decision time); the strategy submits and cancels on every update, exercising the new timing buckets.
- `matching` reflects native book execution, while `message_handling` covers market-event dispatch, logging, and strategy callbacks.
- Throughput improved ~2× over the Python fallback while keeping determinism (digest changes reflect differing fills once the strategy is active).

## Threading & Determinism
- Digests remain stable across repeated runs with identical seeds (e.g. baseline run hash `2af7f0ea...`).
- The concurrent runner accepts pluggable queues (`RingBufferQueue` vs `queue.Queue`) without semantic differences.
- No race-induced divergence observed in replay stress tests.

## Next Steps
1. Explore non-zero latency instrumentation (e.g. synthetic decision delays) so latency averages capture microsecond-scale processing.
2. Capture allocator stats with `valgrind --tool=massif` or `heaptrack` to quantify pool reuse under 100× load.
3. Automate nightly benchmark runs through CI, persisting the JSON output from `python/scripts/run_benchmarks.py` for trend analysis.
