{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1333652a",
   "metadata": {},
   "source": [
    "> Note: Hawkes simulations now call `python.simulate`, which loads the native `hawkes_bridge` shared library from `build/lib`. Before executing simulation cells, run `cmake -S . -B build` and `cmake --build build --target hawkes_bridge`, or set the environment variable `HFT_HAWKES_BRIDGE` to the compiled binary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6be484",
   "metadata": {},
   "source": [
    "# Neural Hawkes Tutorial\n",
    "\n",
    "This notebook demonstrates how to use `neural_hawkes.py` to train a neural surrogate of Hawkes-process dynamics.\n",
    "We cover dataset preparation, training, evaluation, and inspection of predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91a49d2",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "Ensure you have the required dependencies installed:\n",
    "\n",
    "```bash\n",
    "pip install torch numpy matplotlib\n",
    "```\n",
    "\n",
    "The `neural_hawkes.py` script is located at the project root and can be executed as a standalone CLI or imported as a module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0edecc",
   "metadata": {},
   "source": [
    "## 2. Quick start (CLI)\n",
    "\n",
    "The script can be run directly. By default it generates synthetic sequences if no dataset is provided:\n",
    "\n",
    "```bash\n",
    "python neural_hawkes.py --epochs 5 --batch-size 128 --num-types 4 --backbone mlp --mlp-layers 3\n",
    "```\n",
    "\n",
    "Use `--dataset path/to/data.json` to load custom sequences stored as JSON/NPZ (lists of `[timestamp, type]`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390511c2",
   "metadata": {},
   "source": [
    "## 3. Programmatic usage inside Python\n",
    "\n",
    "We can import the key components and run training inside this notebook for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import neural_hawkes as nh\n",
    "\n",
    "# For notebook runs, keep deterministic seeds\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456e557",
   "metadata": {},
   "source": [
    "### 3.1 Generate synthetic sequences\n",
    "\n",
    "`generate_synthetic_sequences` creates IID sequences with exponential inter-arrival times and random event types. Replace this block with your dataset loader if you have real trading data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ba9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequences = nh.generate_synthetic_sequences(num_sequences=100, num_events=200, num_types=4, seed=123)\n",
    "dataset = nh.EventSequenceDataset(sequences, window_size=64, stride=32)\n",
    "train_set, val_set, test_set = nh.split_dataset(dataset, (0.7, 0.15, 0.15))\n",
    "\n",
    "collate = nh.collate_windows\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=False, collate_fn=collate)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b35af4",
   "metadata": {},
   "source": [
    "### 3.2 Instantiate the neural Hawkes model\n",
    "\n",
    "The model combines an event-type embedding, an RNN (GRU or LSTM), and two heads for predicting the next event type and inter-arrival time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7dd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = nh.NeuralHawkesModel(num_types=4, embed_dim=32, hidden_dim=64, backbone='gru').to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6849d973",
   "metadata": {},
   "source": [
    "### 3.3 Training loop\n",
    "\n",
    "We train for a small number of epochs and track loss/accuracy/MAE on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08543acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 5\n",
    "delta_weight = 1.0\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_metrics = nh.train_one_epoch(model, train_loader, optimizer, device, delta_weight)\n",
    "    val_metrics = nh.evaluate(model, val_loader, device, delta_weight)\n",
    "    train_history.append(train_metrics)\n",
    "    val_history.append(val_metrics)\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}: train loss {train_metrics['loss']:.4f} \"\n",
    "        f\"acc {train_metrics['acc']:.4f} mae {train_metrics['mae']:.4f} | \"\n",
    "        f\"val loss {val_metrics['loss']:.4f} acc {val_metrics['acc']:.4f} mae {val_metrics['mae']:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79cbfd6",
   "metadata": {},
   "source": [
    "### 3.4 Evaluation on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95acf3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_metrics = nh.evaluate(model, test_loader, device, delta_weight)\n",
    "print(test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a88acd",
   "metadata": {},
   "source": [
    "## 4. Visualise training curves\n",
    "\n",
    "Plot the training/validation losses and accuracy to inspect convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df968e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = [m['loss'] for m in train_history]\n",
    "val_loss = [m['loss'] for m in val_history]\n",
    "val_acc = [m['acc'] for m in val_history]\n",
    "val_mae = [m['mae'] for m in val_history]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "axes[0].plot(train_loss, label='train loss')\n",
    "axes[0].plot(val_loss, label='val loss')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(val_acc, label='val acc', color='tab:green')\n",
    "axes[1].set_title('Validation accuracy')\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "axes[2].plot(val_mae, label='val MAE', color='tab:orange')\n",
    "axes[2].set_title('Validation MAE')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c93b9cf",
   "metadata": {},
   "source": [
    "## 5. Inspect predictions for a single batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd18fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = next(iter(test_loader))\n",
    "batch = nh.move_batch(batch, device)\n",
    "logits, delta_pred = model(batch['input_types'], batch['input_deltas'], batch['lengths'])\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "print('Predicted event type probabilities (first window):')\n",
    "print(probs[0, :5].cpu().numpy())\n",
    "print('\n",
    "True next event types:')\n",
    "print(batch['target_types'][0, :5].cpu().numpy())\n",
    "print('\n",
    "Predicted inter-arrivals vs true:')\n",
    "print(list(zip(delta_pred[0, :5].detach().cpu().numpy(), batch['target_deltas'][0, :5].cpu().numpy())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fd0720",
   "metadata": {},
   "source": [
    "## 6. Runtime comparison\n",
    "Use helper `measure_runtime` to benchmark CPU vs GPU (when available).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cd3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cpu_time = nh.measure_runtime(model.to('cpu'), test_loader, torch.device('cpu'))\n",
    "print(f'CPU runtime (1 pass): {cpu_time:.3f} s')\n",
    "if torch.cuda.is_available():\n",
    "    gpu_model = nh.NeuralHawkesModel(num_types=4, embed_dim=32, hidden_dim=64)\n",
    "    gpu_model.load_state_dict(model.state_dict())\n",
    "    gpu_time = nh.measure_runtime(gpu_model.to('cuda'), test_loader, torch.device('cuda'))\n",
    "    print(f'GPU runtime (1 pass): {gpu_time:.3f} s')\n",
    "else:\n",
    "    print('GPU not available; skipping GPU timing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c9424",
   "metadata": {},
   "source": [
    "## 7. Saving model checkpoints\n",
    "Optional: save trained weights and optimizer state for reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de75eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_path = Path('neural_hawkes_checkpoint.pth')\n",
    "torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict()}, checkpoint_path)\n",
    "checkpoint_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4c492",
   "metadata": {},
   "source": [
    "## 8. Next steps\n",
    "- Swap the synthetic generator with real trade streams or LOBSTER feeds preprocessed as `(t_i, type_i)`.\n",
    "- Incorporate volumes/marks by extending the dataset and augmenting the model inputs.\n",
    "- Replace the surrogate loss with a proper neural Hawkes likelihood (e.g., log-intensity integration) for better calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f34ee0",
   "metadata": {},
   "source": [
    "## 9. Batch experiments\n",
    "\n",
    "Use `experiments/run_matrix.py` with a JSON config (see `experiments/configs/multi_symbol_example.json`) to sweep across symbols/backbones, then summarise results via `experiments/aggregate_results.py`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
